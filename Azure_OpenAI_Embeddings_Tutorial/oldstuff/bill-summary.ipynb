{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microsoft Tutorial: Explore Azure OpenAI Service embeddings and document search\n",
    "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/tutorials/embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and list models\n",
    "\n",
    "### Supported API versions as of 5/18/2023\n",
    "* 2023-03-15-preview [Swagger spec](https://github.com/Azure/azure-rest-api-specs/blob/main/specification/cognitiveservices/data-plane/AzureOpenAI/inference/preview/2023-03-15-preview/inference.json)\n",
    "* 2022-12-01 [Swagger spec](https://github.com/Azure/azure-rest-api-specs/blob/main/specification/cognitiveservices/data-plane/AzureOpenAI/inference/stable/2022-12-01/inference.json)\n",
    "* ~~2023-05-15 [Swagger spec](https://github.com/Azure/azure-rest-api-specs/blob/main/specification/cognitiveservices/data-plane/AzureOpenAI/inference/stable/2023-05-15/inference.json)~~ *I keep getting 'Resource not found' with this version*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "from num2words import num2words\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "import tiktoken\n",
    "\n",
    "# load_dotenv('C:\\\\Users\\\\BrianOutlaw\\\\git\\\\personal\\\\Learning\\\\Azure_OpenAI_Embeddings_Tutorial\\\\.env')   # Take environment variables from the .env file\n",
    "load_dotenv(os.path.join(os.getcwd(),'.env'))\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\") \n",
    "RESOURCE_ENDPOINT = os.getenv(\"OPENAI_ENDPOINT\") \n",
    "API_VERSION = \"2022-12-01\"\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = API_KEY\n",
    "openai.api_base = RESOURCE_ENDPOINT\n",
    "openai.api_version = API_VERSION\n",
    "\n",
    "url = openai.api_base + \"/openai/deployments?api-version=\" + API_VERSION\n",
    "\n",
    "r = requests.get(url, headers={\"api-key\": API_KEY})\n",
    "\n",
    "print(r.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to read our csv file and create a pandas DataFrame. After the initial DataFrame is created, we can view the contents of the table by running df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(os.path.join(os.getcwd(),'bill_sum_data.csv')) # This assumes that you have placed the bill_sum_data.csv in the same directory you are running Jupyter Notebooks\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial table has more columns than we need we'll create a new smaller DataFrame called df_bills which will contain only the columns for text, summary, and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bills = df[['text', 'summary', 'title']]\n",
    "df_bills"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll perform some light data cleaning by removing redundant whitespace and cleaning up the punctuation to prepare the data for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None #https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#evaluation-order-matters\n",
    "\n",
    "# s is input text\n",
    "def normalize_text(s, sep_token = \" \\n \"):\n",
    "    s = re.sub(r'\\s+',  ' ', s).strip()\n",
    "    s = re.sub(r\". ,\",\"\",s)\n",
    "    # remove all instances of multiple spaces\n",
    "    s = s.replace(\"..\",\".\")\n",
    "    s = s.replace(\". .\",\".\")\n",
    "    s = s.replace(\"\\n\", \"\")\n",
    "    s = s.strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "df_bills['text']= df_bills[\"text\"].apply(lambda x : normalize_text(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to remove any bills that are too long for the token limit (8192 tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "df_bills['n_tokens'] = df_bills[\"text\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "df_bills = df_bills[df_bills.n_tokens<8192]\n",
    "len(df_bills)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll once again examine df_bills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bills"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the n_tokens column a little more as well how text ultimately is tokenized, it can be helpful to run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encode = tokenizer.encode(df_bills.text[0]) \n",
    "decode = tokenizer.decode_tokens_bytes(sample_encode)\n",
    "decode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you then check the length of the decode variable, you'll find it matches the first number in the n_tokens column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(decode)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand more about how tokenization works we can move on to embedding. It is important to note, that we haven't actually tokenized the documents yet. The n_tokens column is simply a way of making sure none of the data we pass to the model for tokenization and embedding exceeds the input token limit of 8,192. When we pass the documents to the embeddings model, it will break the documents into tokens similar (though not necessarily identical) to the examples above and then convert the tokens to a series of floating point numbers that will be accessible via vector search. These embeddings can be stored locally or in an Azure Database. As a result, each bill will have its own corresponding embedding vector in the new ada_v2 column on the right side of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This original code wasn't working, so pivoting to using langchain to get the embedding\n",
    "#df_bills['ada_v2'] = df_bills[\"text\"].apply(lambda x : get_embedding(x, engine = 'text-embedding-ada-002')) # engine should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(deployment=\"ZSuiteEmbeddings\")\n",
    "\n",
    "embeddings_array = []\n",
    "# Iterate through each bill\n",
    "for index, row in df_bills.iterrows():\n",
    "    # Generate a vector for the bill text\n",
    "    query_result = embeddings.embed_query(row['text'])\n",
    "    embeddings_array.append(query_result)\n",
    "\n",
    "# Add all the vectors to the dataframe\n",
    "df_bills['ada_v2']= embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bills"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following didn't work and I have not fixed it yet\n",
    "\n",
    "As we run the search code block below, we'll embed the search query \"Can I get information on cable company tax revenue?\" with the same text-embedding-ada-002 (Version 2) model. Next we'll find the closest bill embedding to the newly embedded text from our query ranked by cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # search through the reviews for a specific product\n",
    "# def search_docs(df, user_query, top_n=3, to_print=True):\n",
    "#     embedding = get_embedding(\n",
    "#         user_query,\n",
    "#         engine=\"text-embedding-ada-002\" # engine should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\n",
    "#     )\n",
    "#     df[\"similarities\"] = df.ada_v2.apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "#     res = (\n",
    "#         df.sort_values(\"similarities\", ascending=False)\n",
    "#         .head(top_n)\n",
    "#     )\n",
    "#     if to_print:\n",
    "#         display(res)\n",
    "#     return res\n",
    "\n",
    "\n",
    "# res = search_docs(df_bills, \"Can I get information on cable company tax revenue?\", top_n=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
